{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We import 'Document' - this is the specific format LangChain uses to store text.\n",
    "# 2. We import 'TextSplitters' - these are tools used to chop long text into smaller pieces.\n",
    "# 3. We import 'pandas' - a popular tool for viewing data in organized tables.\n",
    "\n",
    "import langchain\n",
    "import os   \n",
    "from typing import List,Dict,Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d868ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\02KrishRAGBootCamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (RecursiveCharacterTextSplitter, CharacterTextSplitter, TokenTextSplitter)\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec35297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Structure\n",
      "Content :This is the main text content that will be embedded and searched.\n",
      "Metadata :{'source': 'example.txt', 'page': 1, 'author': 'Mathew', 'date_created': '2024-01-01', 'cutom_field': 'any_value'}\n",
      "\n",
      "üìù Metadata is crucial for:\n",
      "- Filtering search results\n",
      "- Tracking document sources\n",
      "- Providing context in responses\n",
      "- Debugging and auditing\n"
     ]
    }
   ],
   "source": [
    "# 1. A 'Document' is like a package: it contains the 'page_content' (the text) and 'metadata' (the tags).\n",
    "# 2. Metadata is crucial for \"Filtering\" (e.g., searching only for documents written by a specific author).\n",
    "# 3. Metadata is also used for \"Auditing\" (e.g., checking exactly when a piece of information was added).\n",
    "# 4. In LangChain, we always convert our data into this format before doing anything else.\n",
    "\n",
    "## create a simple document\n",
    "doc=Document(\n",
    "    page_content=\"This is the main text content that will be embedded and searched.\",\n",
    "    metadata={\n",
    "        \"source\":\"example.txt\",\n",
    "        \"page\":1,\n",
    "        \"author\":\"Mathew\",\n",
    "        \"date_created\":\"2024-01-01\",\n",
    "        \"cutom_field\":\"any_value\"\n",
    "\n",
    "    }\n",
    ")\n",
    "print(\"Document Structure\")\n",
    "\n",
    "print(f\"Content :{doc.page_content}\")\n",
    "print(f\"Metadata :{doc.metadata}\")\n",
    "\n",
    "# Why metadata matters:\n",
    "print(\"\\n Metadata is crucial for:\")\n",
    "print(\"- Filtering search results\")\n",
    "print(\"- Tracking document sources\")\n",
    "print(\"- Providing context in responses\")\n",
    "print(\"- Debugging and auditing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a26ad15",
   "metadata": {},
   "source": [
    "### Text Files (.txt) - The Simplest Case {#2-text-files}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49269f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 'os.makedirs' creates a folder on your computer to hold your data.\n",
    "# 2. 'sample_texts' is a dictionary containing the text we want to save into files.\n",
    "# 3. The 'for' loop goes through that dictionary and physically writes the text into .txt files.\n",
    "# 4. This step prepares our \"raw data\" so we have something to practice loading later.\n",
    "\n",
    "os.makedirs(\"data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268f916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3f15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document\n",
      "Content preview: Python Programming Introduction\n",
      "\n",
      "Python is a high-level, interpreted programming language known for ...\n",
      "Metadata: {'source': 'data/text_files/python_intro.txt'}\n"
     ]
    }
   ],
   "source": [
    "# 1. 'TextLoader' is a tool specifically designed to read standard text (.txt) files.\n",
    "# 2. '.load()' is the trigger command that turns the file into a LangChain Document object.\n",
    "# 3. 'page_content' is the actual text inside the file.\n",
    "# 4. 'metadata' is \"extra info\" like the file name or location, which helps the AI cite its sources.\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "## Loading a single text file\n",
    "loader=TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "documents=loader.load()\n",
    "print(f\"Loaded {len(documents)} document\")\n",
    "print(f\"Content preview: {documents[0].page_content[:100]}...\")\n",
    "print(f\"Metadata: {documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 48.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loaded 2 documents\n",
      "\n",
      "Document 1:\n",
      "  Source: data\\text_files\\machine_learning.txt\n",
      "  Length: 575 characters\n",
      "\n",
      "Document 2:\n",
      "  Source: data\\text_files\\python_intro.txt\n",
      "  Length: 489 characters\n",
      "\n",
      "üìä DirectoryLoader Characteristics:\n",
      "‚úÖ Advantages:\n",
      "  - Loads multiple files at once\n",
      "  - Supports glob patterns\n",
      "  - Progress tracking\n",
      "  - Recursive directory scanning\n",
      "\n",
      "‚ùå Disadvantages:\n",
      "  - All files must be same type\n",
      "  - Limited error handling per file\n",
      "  - Can be memory intensive for large directories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 'DirectoryLoader' is used when you have many files and don't want to load them one by one.\n",
    "# 2. 'glob=\"**/*.txt\"' tells the loader to find every .txt file in the folder and its sub-folders.\n",
    "# 3. 'loader_cls=TextLoader' tells this tool to use our TextLoader to open every file it finds.\n",
    "# 4. 'show_progress=True' adds a visual bar so you can see how many files are finished loading.\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=True\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "\n",
    "print(f\"üìÅ Loaded {len(documents)} documents\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"  Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Length: {len(doc.page_content)} characters\")\n",
    "\n",
    "\n",
    "# üìä Analysis\n",
    "print(\"\\nüìä DirectoryLoader Characteristics:\")\n",
    "print(\"‚úÖ Advantages:\")\n",
    "print(\"  - Loads multiple files at once\")\n",
    "print(\"  - Supports glob patterns\")\n",
    "print(\"  - Progress tracking\")\n",
    "print(\"  - Recursive directory scanning\")\n",
    "\n",
    "print(\"\\n‚ùå Disadvantages:\")\n",
    "print(\"  - All files must be same type\")\n",
    "print(\"  - Limited error handling per file\")\n",
    "print(\"  - Can be memory intensive for large directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25047e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '), Document(metadata={'source': 'data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "# 1. This cell introduces 'Text Splitting', which is the process of breaking long documents \n",
    "#    into smaller, bite-sized pieces (chunks) so the AI can process them more easily.\n",
    "# 2. 'CharacterTextSplitter': Splits text based on a specific number of characters (very simple).\n",
    "# 3. 'RecursiveCharacterTextSplitter': The most recommended splitter. It tries to split \n",
    "#    at natural points like paragraphs and sentences so the meaning isn't lost.\n",
    "# 4. 'TokenTextSplitter': Splits text based on \"tokens\" (how AI models actually count words), \n",
    "#    ensuring you stay within the AI's memory limits.\n",
    "# 5. 'print(documents)': This displays your currently loaded data to confirm it's ready for splitting.\n",
    "### Different text splitting strategies\n",
    "from langchain_text_splitters import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7b354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 'documents[0]' selects the very first Document object from the list you loaded earlier.\n",
    "# 2. '.page_content' tells Python to ignore the metadata (like filename or author) \n",
    "#    and only grab the actual raw text inside that document.\n",
    "# 3. We store this raw text in the variable 'text' so we can use it to test \n",
    "#    the \"Character Text Splitter\" method.\n",
    "# 4. Typing 'text' on the last line simply displays the content so you can \n",
    "#    see exactly what you are about to split.\n",
    "\n",
    "### MEthod 1- Character Text Splitter\n",
    "text=documents[0].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d9d4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPARISON OF CHUNKING STRATEGIES ---\n",
    "\n",
    "# 1. CHARACTER SPLITTER (The \"Rigid\" Method)\n",
    "#    - How it works: It looks for ONE specific character (like a space) and cuts \n",
    "#      exactly when it hits the limit.\n",
    "#    - Pros: Very simple and predictable.\n",
    "#    - Cons: It's \"blind\" to the structure of your writing. It might cut a \n",
    "#      sentence right in the middle just because it reached the character limit.\n",
    "\n",
    "# 2. RECURSIVE CHARACTER SPLITTER (The \"Smart\" Method)\n",
    "#    - How it works: It has a hierarchy of separators (Paragraphs -> Sentences -> Words).\n",
    "#      If a paragraph is too long, it tries to split at a sentence. If the sentence \n",
    "#      is too long, it tries to split at a word.\n",
    "#    - Pros: It tries its best to keep related text together so the AI can \n",
    "#      understand the full context of a thought.\n",
    "#    - Cons: Slightly more complex, but usually the best choice for general text.\n",
    "\n",
    "# 3. TOKEN SPLITTER (The \"Technical\" Method)\n",
    "#    - How it works: It doesn't count letters; it counts \"Tokens\" (the chunks \n",
    "#      of characters that AI models use to process language).\n",
    "#    - Pros: Most accurate for AI memory management. Since AI models have a \n",
    "#      \"token limit,\" this ensures you never send too much data at once.\n",
    "#    - Cons: Hard for humans to visualize because tokens don't always \n",
    "#      align perfectly with word or character counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c4fc6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ CHARACTER TEXT SPLITTER\n",
      "Created 3 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# 1. 'separator=\" \"': This tells the splitter to only cut the text at a space. \n",
    "#    It prevents the code from cutting a word right in the middle.\n",
    "# 2. 'chunk_size=200': This is our goal length. We want each piece of text \n",
    "#    to be roughly 200 characters long.\n",
    "# 3. 'chunk_overlap=20': This is very important! It takes 20 characters from the end \n",
    "#    of Chunk 1 and repeats them at the start of Chunk 2. This helps the AI \n",
    "#    keep the context between chunks so it doesn't \"forget\" the beginning of a sentence.\n",
    "# 4. 'length_function=len': This tells the tool to use standard Python counting \n",
    "#    (1 character = 1 unit) to measure the size.\n",
    "# 5. '.split_text(text)': This is the action command that takes your long string \n",
    "#    and returns a list of smaller strings (chunks).\n",
    "# 6. The print statements help you verify how many pieces were created and \n",
    "#    show you a preview of the first one.\n",
    "# Method 1: Character-based splitting\n",
    "print(\"1Ô∏è‚É£ CHARACTER TEXT SPLITTER\")\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",  # Split on newlines\n",
    "    chunk_size=200,  # Max chunk size in characters\n",
    "    chunk_overlap=20,  # Overlap between chunks\n",
    "    length_function=len  # How to measure chunk size\n",
    ")\n",
    "\n",
    "char_chunks=char_splitter.split_text(text)\n",
    "print(f\"Created {len(char_chunks)} chunks\")\n",
    "print(f\"First chunk: {char_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf3aa81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing\n",
      "------------------\n",
      "on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning:\n"
     ]
    }
   ],
   "source": [
    "print(char_chunks[0])\n",
    "print(\"------------------\")\n",
    "print(char_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d277d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ RECURSIVE CHARACTER TEXT SPLITTER\n",
      "Created 4 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# 1. 'Recursive' means the splitter is \"smart.\" It tries to split text using a list of \n",
    "#    separators in order (like paragraphs, then sentences, then words) to keep ideas together.\n",
    "# 2. 'separators=[\" \"]': In this specific code, it is told to look for spaces. However, \n",
    "#    by default, this splitter usually looks for double newlines, then single newlines, then spaces.\n",
    "# 3. 'chunk_size=200' & 'chunk_overlap=20': Just like the previous method, we want \n",
    "#    200-character pieces with 20 characters of \"shared memory\" between them.\n",
    "# 4. Why it is RECOMMENDED: Unlike the basic character splitter, this one tries its best \n",
    "#    NOT to break a paragraph or a sentence in the middle unless it absolutely has to.\n",
    "# 5. '.split_text(text)': It processes the raw text and turns it into a list of \n",
    "#    well-organized chunks.\n",
    "\n",
    "# Method 2: Recursive character splitting (RECOMMENDED)\n",
    "print(\"\\n2Ô∏è‚É£ RECURSIVE CHARACTER TEXT SPLITTER\")\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],  # Try these separators in order\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "print(f\"Created {len(recursive_chunks)} chunks\")\n",
    "print(f\"First chunk: {recursive_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7da7c2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
      "from experience without being explicitly programmed. It focuses on developing\n",
      "-----------------\n",
      "on developing computer programs\n",
      "that can access data and use it to learn for themselves.\n",
      "\n",
      "Types of Machine Learning:\n",
      "1. Supervised Learning: Learning with labeled data\n",
      "2. Unsupervised Learning:\n",
      "------------------\n",
      "Learning: Finding patterns in unlabeled data\n",
      "3. Reinforcement Learning: Learning through rewards and penalties\n",
      "\n",
      "Applications include image recognition, speech processing, and recommendation\n"
     ]
    }
   ],
   "source": [
    "print(recursive_chunks[0])\n",
    "print(\"-----------------\")\n",
    "print(recursive_chunks[1])\n",
    "print(\"------------------\")\n",
    "print(recursive_chunks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941cc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple text example - 4 chunks:\n",
      "\n",
      "Chunk 1: 'This is sentence one and it is quite long. This is sentence two and it is also'\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "\n",
      "Chunk 2: 'two and it is also quite long. This is sentence three which is even longer than'\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "\n",
      "Chunk 3: 'is even longer than the others. This is sentence four. This is sentence five.'\n",
      "Chunk 4: 'is sentence five. This is sentence six.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 'simple_text': We create a single long line of text with NO newlines. \n",
    "#    This forces the splitter to find spaces to break the text.\n",
    "# 2. 'separators=[\" \"]': We are overriding the default settings to tell the tool \n",
    "#    \"ONLY split where there is a space.\" \n",
    "# 3. 'chunk_size=80': We want small, manageable pieces of about 80 characters each.\n",
    "# 4. 'chunk_overlap=20': This ensures the last 20 characters of Chunk 1 \n",
    "#    appear at the beginning of Chunk 2.\n",
    "# 5. The 'for' loop: This is designed to print chunks side-by-side so you can \n",
    "#    visually verify that the end of one chunk matches the start of the next.\n",
    "# 6. This experiment proves that even if you have no paragraphs, the splitter \n",
    "#    will intelligently use spaces to keep words whole while maintaining \"memory\" (overlap).\n",
    "# Create text without natural break points\n",
    "simple_text = \"This is sentence one and it is quite long. This is sentence two and it is also quite long. This is sentence three which is even longer than the others. This is sentence four. This is sentence five. This is sentence six.\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\" \"],  # Only split on spaces\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(simple_text)\n",
    "\n",
    "print(f\"\\nSimple text example - {len(chunks)} chunks:\\n\")\n",
    "\n",
    "for i in range(len(chunks) - 1):\n",
    "    print(f\"Chunk {i+1}: '{chunks[i]}'\")\n",
    "    print(f\"Chunk {i+2}: '{chunks[i+1]}'\")\n",
    "    \n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681fa1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ TOKEN TEXT SPLITTER\n",
      "Created 3 chunks\n",
      "First chunk: Machine Learning Basics\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables system...\n"
     ]
    }
   ],
   "source": [
    "# 1. 'Tokens' are not the same as characters. While characters are individual letters, \n",
    "#    tokens are chunks of characters (roughly 4 characters or 0.75 words) that AI models \n",
    "#    use to \"read\" and process language.\n",
    "# 2. 'chunk_size=50': Unlike the previous cells that used characters, this tells the \n",
    "#    tool to create chunks that are exactly 50 \"tokens\" long.\n",
    "# 3. 'chunk_overlap=10': This keeps 10 tokens of shared context between the chunks.\n",
    "# 4. Why use this? AI models (like GPT) have a \"context limit\" measured in tokens. \n",
    "#    Using a TokenSplitter ensures your data fits perfectly into the AI's \"memory\" \n",
    "#    without accidentally going over the limit.\n",
    "# 5. This method is often preferred when you are trying to be very precise about \n",
    "#    how much information you are sending to the AI model at once.\n",
    "\n",
    "# Method 3: Token-based splitting\n",
    "print(\"\\n3Ô∏è‚É£ TOKEN TEXT SPLITTER\")\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,  # Size in tokens (not characters)\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "print(f\"Created {len(token_chunks)} chunks\")\n",
    "print(f\"First chunk: {token_chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29e1e06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Text Splitting Methods Comparison:\n",
      "\n",
      "CharacterTextSplitter:\n",
      "  ‚úÖ Simple and predictable\n",
      "  ‚úÖ Good for structured text\n",
      "  ‚ùå May break mid-sentence\n",
      "  Use when: Text has clear delimiters\n",
      "\n",
      "RecursiveCharacterTextSplitter:\n",
      "  ‚úÖ Respects text structure\n",
      "  ‚úÖ Tries multiple separators\n",
      "  ‚úÖ Best general-purpose splitter\n",
      "  ‚ùå Slightly more complex\n",
      "  Use when: Default choice for most texts\n",
      "\n",
      "TokenTextSplitter:\n",
      "  ‚úÖ Respects model token limits\n",
      "  ‚úÖ More accurate for embeddings\n",
      "  ‚ùå Slower than character-based\n",
      "  Use when: Working with token-limited models\n"
     ]
    }
   ],
   "source": [
    "# üìä Comparison\n",
    "print(\"\\nüìä Text Splitting Methods Comparison:\")\n",
    "print(\"\\nCharacterTextSplitter:\")\n",
    "print(\"  ‚úÖ Simple and predictable\")\n",
    "print(\"  ‚úÖ Good for structured text\")\n",
    "print(\"  ‚ùå May break mid-sentence\")\n",
    "print(\"  Use when: Text has clear delimiters\")\n",
    "\n",
    "print(\"\\nRecursiveCharacterTextSplitter:\")\n",
    "print(\"  ‚úÖ Respects text structure\")\n",
    "print(\"  ‚úÖ Tries multiple separators\")\n",
    "print(\"  ‚úÖ Best general-purpose splitter\")\n",
    "print(\"  ‚ùå Slightly more complex\")\n",
    "print(\"  Use when: Default choice for most texts\")\n",
    "\n",
    "print(\"\\nTokenTextSplitter:\")\n",
    "print(\"  ‚úÖ Respects model token limits\")\n",
    "print(\"  ‚úÖ More accurate for embeddings\")\n",
    "print(\"  ‚ùå Slower than character-based\")\n",
    "print(\"  Use when: Working with token-limited models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02krishragbootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
